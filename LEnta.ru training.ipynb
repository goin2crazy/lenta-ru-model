{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":186280444,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"class Config: \n    datapath = '/kaggle/input/lenta-ru-private-dataset-for-tatar-hackathon/lenta_ru_news_2019_2023.csv'\n    \n    ner_preset = \"Davlan/distilbert-base-multilingual-cased-ner-hrl\"\n    sentiment_preset = 'cointegrated/rubert-tiny-sentiment-balanced'\n\n#     preprocessing\n    bias = 0.2 \n    test_size = 0.25\n    \n#     training\n    batch_size = 64\n    \ncfg = Config() ","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:03:13.562039Z","iopub.execute_input":"2024-07-02T12:03:13.562432Z","iopub.status.idle":"2024-07-02T12:03:13.568141Z","shell.execute_reply.started":"2024-07-02T12:03:13.562401Z","shell.execute_reply":"2024-07-02T12:03:13.567043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install -q evaluate\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom transformers import (AutoTokenizer, \n                          AutoModelForTokenClassification, \n                          AutoModelForSequenceClassification, \n                          pipeline\n                         )\nfrom sklearn.model_selection import train_test_split\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n\nos.environ['HF_TOKEN'] = secret_value_0\n\nfrom tqdm import tqdm \ntqdm.pandas() \n\nimport wandb\nwandb.init(mode='disabled')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-02T12:03:13.569617Z","iopub.execute_input":"2024-07-02T12:03:13.569902Z","iopub.status.idle":"2024-07-02T12:03:26.783972Z","shell.execute_reply.started":"2024-07-02T12:03:13.569877Z","shell.execute_reply":"2024-07-02T12:03:26.782929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_files = list() \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        data_files.append(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:03:26.785737Z","iopub.execute_input":"2024-07-02T12:03:26.786042Z","iopub.status.idle":"2024-07-02T12:03:26.793205Z","shell.execute_reply.started":"2024-07-02T12:03:26.786014Z","shell.execute_reply":"2024-07-02T12:03:26.792272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(cfg.sentiment_preset)\nmodel = AutoModelForSequenceClassification.from_pretrained(cfg.sentiment_preset)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:03:26.794461Z","iopub.execute_input":"2024-07-02T12:03:26.794771Z","iopub.status.idle":"2024-07-02T12:03:27.064533Z","shell.execute_reply.started":"2024-07-02T12:03:26.794747Z","shell.execute_reply":"2024-07-02T12:03:27.063471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA","metadata":{}},{"cell_type":"code","source":"import ast \nimport datasets \n\ndef mean(i): \n    try: \n        return sum(i)/len(i) + cfg.bias\n    except: \n        return 0\n\ndef get_inputs_outputs(item): \n    text = item['text']\n    organization_sentiments = item['organizations_sentiments']\n    organization_sentiments = ast.literal_eval(organization_sentiments)\n    \n    return (\n        # inputs\n        [f\"[focus: {i['word']}] \\n{text}\" for i in organization_sentiments], \n        \n        # outputs\n        [mean(i['sentiment']) for i in organization_sentiments]\n    )","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:03:27.066752Z","iopub.execute_input":"2024-07-02T12:03:27.067106Z","iopub.status.idle":"2024-07-02T12:03:27.074436Z","shell.execute_reply.started":"2024-07-02T12:03:27.067068Z","shell.execute_reply":"2024-07-02T12:03:27.073444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pp1(item):\n    if item['score'] > .5: \n        label = 2\n    elif item['score'] < .5 and item['score'] > -.5: \n        label = 1 \n    elif item['score'] < -.5: \n        label = 0 \n    \n    return {\n        'label': torch.tensor(int(label))\n    }\n\ndef pp2(item):\n    return tokenizer(item['text'], return_tensors='pt', padding=True, truncation=True)\n\ndef to_dataset(X, y): \n    ds = datasets.Dataset.from_dict({'text': X, 'score': y})\n    \n    return (ds\n            .map(pp1)\n            .map(pp2, batched = True))\n\ndef train_val_datasets(X, y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size= cfg.test_size)\n    \n    return to_dataset(X_train, y_train), to_dataset(X_val, y_val)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:03:27.075662Z","iopub.execute_input":"2024-07-02T12:03:27.075961Z","iopub.status.idle":"2024-07-02T12:03:27.089478Z","shell.execute_reply.started":"2024-07-02T12:03:27.075937Z","shell.execute_reply":"2024-07-02T12:03:27.088652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntraining_args = TrainingArguments(\n    output_dir=\"lenta-ru-sentiments\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=cfg.batch_size,\n    per_device_eval_batch_size=cfg.batch_size,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=True,\n    save_total_limit = 1, \n)\n\ndef start_train(train_dataset, eval_dataset): \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:03:27.090590Z","iopub.execute_input":"2024-07-02T12:03:27.090863Z","iopub.status.idle":"2024-07-02T12:03:27.124546Z","shell.execute_reply.started":"2024-07-02T12:03:27.090839Z","shell.execute_reply":"2024-07-02T12:03:27.123597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:04:57.127690Z","iopub.execute_input":"2024-07-02T12:04:57.128017Z","iopub.status.idle":"2024-07-02T12:04:57.135565Z","shell.execute_reply.started":"2024-07-02T12:04:57.127991Z","shell.execute_reply":"2024-07-02T12:04:57.134619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\n\naccuracy = evaluate.load(\"accuracy\")\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:03:27.125916Z","iopub.execute_input":"2024-07-02T12:03:27.126340Z","iopub.status.idle":"2024-07-02T12:03:27.582278Z","shell.execute_reply.started":"2024-07-02T12:03:27.126303Z","shell.execute_reply":"2024-07-02T12:03:27.581294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for f in data_files: \n    try: \n        df = pd.read_csv(f)\n    except: \n        break\n    X = list() \n    y = list() \n    \n    for i in range(df.shape[0]): \n        item = df.iloc[i]\n        \n        inputs, outputs = get_inputs_outputs(item)\n        X.extend(inputs)\n        y.extend(outputs)\n    \n    train_dataset, eval_dataset = train_val_datasets(X, y)\n    start_train(train_dataset, eval_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T12:03:27.583969Z","iopub.execute_input":"2024-07-02T12:03:27.584619Z","iopub.status.idle":"2024-07-02T12:04:40.769591Z","shell.execute_reply.started":"2024-07-02T12:03:27.584581Z","shell.execute_reply":"2024-07-02T12:04:40.768189Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
